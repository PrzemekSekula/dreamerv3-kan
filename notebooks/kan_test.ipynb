{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions as torchd\n",
    "\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp_dim,\n",
    "        shape,\n",
    "        layers,\n",
    "        units,\n",
    "        act=\"SiLU\",\n",
    "        norm=True,\n",
    "        dist=\"normal\",\n",
    "        std=1.0,\n",
    "        min_std=0.1,\n",
    "        max_std=1.0,\n",
    "        absmax=None,\n",
    "        temp=0.1,\n",
    "        unimix_ratio=0.01,\n",
    "        outscale=1.0,\n",
    "        symlog_inputs=False,\n",
    "        device=\"cuda\",\n",
    "        name=\"NoName\",\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        self._shape = (shape,) if isinstance(shape, int) else shape\n",
    "        if self._shape is not None and len(self._shape) == 0:\n",
    "            self._shape = (1,)\n",
    "        act = getattr(torch.nn, act)\n",
    "        self._dist = dist\n",
    "        self._std = std if isinstance(std, str) else torch.tensor((std,), device=device)\n",
    "        self._min_std = min_std\n",
    "        self._max_std = max_std\n",
    "        self._absmax = absmax\n",
    "        self._temp = temp\n",
    "        self._unimix_ratio = unimix_ratio\n",
    "        self._symlog_inputs = symlog_inputs\n",
    "        self._device = device\n",
    "\n",
    "        self.layers = nn.Sequential()\n",
    "        for i in range(layers):\n",
    "            self.layers.add_module(\n",
    "                f\"{name}_linear{i}\", nn.Linear(inp_dim, units, bias=False)\n",
    "            )\n",
    "            if norm:\n",
    "                self.layers.add_module(\n",
    "                    f\"{name}_norm{i}\", nn.LayerNorm(units, eps=1e-03)\n",
    "                )\n",
    "            self.layers.add_module(f\"{name}_act{i}\", act())\n",
    "            if i == 0:\n",
    "                inp_dim = units\n",
    "        self.layers.apply(tools.weight_init)\n",
    "\n",
    "        if isinstance(self._shape, dict):\n",
    "            self.mean_layer = nn.ModuleDict()\n",
    "            for name, shape in self._shape.items():\n",
    "                self.mean_layer[name] = nn.Linear(inp_dim, np.prod(shape))\n",
    "            self.mean_layer.apply(tools.uniform_weight_init(outscale))\n",
    "            if self._std == \"learned\":\n",
    "                assert dist in (\"tanh_normal\", \"normal\", \"trunc_normal\", \"huber\"), dist\n",
    "                self.std_layer = nn.ModuleDict()\n",
    "                for name, shape in self._shape.items():\n",
    "                    self.std_layer[name] = nn.Linear(inp_dim, np.prod(shape))\n",
    "                self.std_layer.apply(tools.uniform_weight_init(outscale))\n",
    "        elif self._shape is not None:\n",
    "            self.mean_layer = nn.Linear(inp_dim, np.prod(self._shape))\n",
    "            self.mean_layer.apply(tools.uniform_weight_init(outscale))\n",
    "            if self._std == \"learned\":\n",
    "                assert dist in (\"tanh_normal\", \"normal\", \"trunc_normal\", \"huber\"), dist\n",
    "                self.std_layer = nn.Linear(units, np.prod(self._shape))\n",
    "                self.std_layer.apply(tools.uniform_weight_init(outscale))\n",
    "\n",
    "    def forward(self, features, dtype=None):\n",
    "        x = features\n",
    "        if self._symlog_inputs:\n",
    "            x = tools.symlog(x)\n",
    "        out = self.layers(x)\n",
    "        # Used for encoder output\n",
    "        if self._shape is None:\n",
    "            return out\n",
    "        if isinstance(self._shape, dict):\n",
    "            dists = {}\n",
    "            for name, shape in self._shape.items():\n",
    "                mean = self.mean_layer[name](out)\n",
    "                if self._std == \"learned\":\n",
    "                    std = self.std_layer[name](out)\n",
    "                else:\n",
    "                    std = self._std\n",
    "                dists.update({name: self.dist(self._dist, mean, std, shape)})\n",
    "            return dists\n",
    "        else:\n",
    "            mean = self.mean_layer(out)\n",
    "            if self._std == \"learned\":\n",
    "                std = self.std_layer(out)\n",
    "            else:\n",
    "                std = self._std\n",
    "            return self.dist(self._dist, mean, std, self._shape)\n",
    "\n",
    "    def dist(self, dist, mean, std, shape):\n",
    "        if dist == \"tanh_normal\":\n",
    "            mean = torch.tanh(mean)\n",
    "            std = F.softplus(std) + self._min_std\n",
    "            dist = torchd.normal.Normal(mean, std)\n",
    "            dist = torchd.transformed_distribution.TransformedDistribution(\n",
    "                dist, tools.TanhBijector()\n",
    "            )\n",
    "            dist = torchd.independent.Independent(dist, 1)\n",
    "            dist = tools.SampleDist(dist)\n",
    "        elif dist == \"normal\":\n",
    "            std = (self._max_std - self._min_std) * torch.sigmoid(\n",
    "                std + 2.0\n",
    "            ) + self._min_std\n",
    "            dist = torchd.normal.Normal(torch.tanh(mean), std)\n",
    "            dist = tools.ContDist(\n",
    "                torchd.independent.Independent(dist, 1), absmax=self._absmax\n",
    "            )\n",
    "        elif dist == \"normal_std_fixed\":\n",
    "            dist = torchd.normal.Normal(mean, self._std)\n",
    "            dist = tools.ContDist(\n",
    "                torchd.independent.Independent(dist, 1), absmax=self._absmax\n",
    "            )\n",
    "        elif dist == \"trunc_normal\":\n",
    "            mean = torch.tanh(mean)\n",
    "            std = 2 * torch.sigmoid(std / 2) + self._min_std\n",
    "            dist = tools.SafeTruncatedNormal(mean, std, -1, 1)\n",
    "            dist = tools.ContDist(\n",
    "                torchd.independent.Independent(dist, 1), absmax=self._absmax\n",
    "            )\n",
    "        elif dist == \"onehot\":\n",
    "            dist = tools.OneHotDist(mean, unimix_ratio=self._unimix_ratio)\n",
    "        elif dist == \"onehot_gumble\":\n",
    "            dist = tools.ContDist(\n",
    "                torchd.gumbel.Gumbel(mean, 1 / self._temp), absmax=self._absmax\n",
    "            )\n",
    "        elif dist == \"huber\":\n",
    "            dist = tools.ContDist(\n",
    "                torchd.independent.Independent(\n",
    "                    tools.UnnormalizedHuber(mean, std, 1.0),\n",
    "                    len(shape),\n",
    "                    absmax=self._absmax,\n",
    "                )\n",
    "            )\n",
    "        elif dist == \"binary\":\n",
    "            dist = tools.Bernoulli(\n",
    "                torchd.independent.Independent(\n",
    "                    torchd.bernoulli.Bernoulli(logits=mean), len(shape)\n",
    "                )\n",
    "            )\n",
    "        elif dist == \"symlog_disc\":\n",
    "            dist = tools.DiscDist(logits=mean, device=self._device)\n",
    "        elif dist == \"symlog_mse\":\n",
    "            dist = tools.SymlogDist(mean)\n",
    "        else:\n",
    "            raise NotImplementedError(dist)\n",
    "        return dist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreamer_impera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
