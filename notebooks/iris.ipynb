{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAN for IRIS\n",
    "\n",
    "Iris dataset classification with Kolmogorov-Arnold networks. For learning purpsoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from kan.utils import create_dataset_from_data\n",
    "from kan import KAN \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cuda:9'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "dtype = torch.get_default_dtype()\n",
    "print (\"Dtype:\", dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)  # Ensure float32 for numerical data\n",
    "y = torch.tensor(y, dtype=torch.long).unsqueeze(1)\n",
    "\n",
    "dataset = create_dataset_from_data(X, y, device=device)\n",
    "\n",
    "for key, value in dataset.items():\n",
    "    print(key, value.shape)\n",
    "\n",
    "nr_features = dataset['train_input'].shape[1]\n",
    "print (f'We have {nr_features} features.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression - not recommended\n",
    "Let's approach this as a regression problem first. I do not recommend doing classification this way, it was just something that came up during learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = KAN(width=[nr_features,16, 16,1], grid=5, k=3, seed=0).to(device)\n",
    "\n",
    "def train_acc_reg():\n",
    "    return torch.mean((torch.round(model_reg(dataset['train_input'])[:,0]) == dataset['train_label'][:,0]).float())\n",
    "\n",
    "def test_acc_reg():\n",
    "    return torch.mean((torch.round(model_reg(dataset['test_input'])[:,0]) == dataset['test_label'][:,0]).float())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_reg.fit(\n",
    "    dataset, \n",
    "    opt=\"LBFGS\", \n",
    "    steps=10, \n",
    "    metrics=(train_acc_reg, test_acc_reg)\n",
    "    )\n",
    "\n",
    "print(results['train_acc_reg'][-1], results['test_acc_reg'][-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Now we can do classification, typical softmax on the last layer.\n",
    "\n",
    "Notes:\n",
    "- We do not add softmax to the last layer, it is added by CrossEntropyLoss()\n",
    "- We are rebuilding dataset. The trick is, for regression we wanted labes to be unsqueezed to size `[n, 1]`, and now we want a vector `[n]` where `n` stands for the number of examples (rows) in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dataset['train_label'].shape) == 2:\n",
    "    dataset['train_label'] = dataset['train_label'].squeeze()\n",
    "    dataset['test_label'] = dataset['test_label'].squeeze()\n",
    "\n",
    "for key, value in dataset.items():\n",
    "    print(key, value.shape)\n",
    "\n",
    "nr_features = dataset['train_input'].shape[1]\n",
    "nr_classes = y.unique().shape[0]\n",
    "print (f'We have {nr_features} features and {nr_classes} classes.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN(\n",
    "    width=[nr_features,16, 8,nr_classes], \n",
    "    grid=5, \n",
    "    k=3, \n",
    "    seed=0\n",
    "    ).to(device)\n",
    "\n",
    "def train_acc():\n",
    "    return torch.mean((torch.argmax(model(\n",
    "        dataset['train_input']), dim=1) == dataset['train_label']).type(dtype))\n",
    "\n",
    "def test_acc():\n",
    "    return torch.mean((torch.argmax(model(\n",
    "        dataset['test_input']), dim=1) == dataset['test_label']).type(dtype))\n",
    "\n",
    "\n",
    "results = model.fit(\n",
    "    dataset, \n",
    "    opt=\"LBFGS\", \n",
    "    steps=10, \n",
    "    metrics=(train_acc, test_acc), \n",
    "    loss_fn=torch.nn.CrossEntropyLoss()\n",
    "    )\n",
    "\n",
    "print(results['train_acc'][-1], results['test_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are logged after every step. Example:\n",
    "print ('Train: ', results['train_acc'])\n",
    "print ('Test: ', results['test_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with adam learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN(\n",
    "    width=[nr_features,16, 8,nr_classes], \n",
    "    grid=5, \n",
    "    k=3, \n",
    "    seed=0\n",
    "    ).to(device)    \n",
    "\n",
    "model.fit(\n",
    "    dataset, \n",
    "    opt=\"Adam\", \n",
    "    lr=1e-3, \n",
    "    steps=50, \n",
    "    #lamb=1e-3, \n",
    "    #lamb_entropy=5., \n",
    "    #update_grid=False\n",
    "    metrics=(train_acc, test_acc), \n",
    "    loss_fn=torch.nn.CrossEntropyLoss()    \n",
    "    )\n",
    "\n",
    "print(results['train_acc'][-1], results['test_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with batches\n",
    "You can learn with batches using native `KAN.fit()` but it works weird, and I did not figure out why. \n",
    "Anyway, to implement it into Dreamer we will have to integrate this with Torch, so let's check if we can do it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8\n",
    "train_dataset = TensorDataset(dataset['train_input'],  dataset['train_label'])\n",
    "test_dataset = TensorDataset(dataset['test_input'], dataset['test_label'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the model on data from data_loader\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        acc = correct / total * 100\n",
    "    return acc\n",
    "\n",
    "\n",
    "model = KAN(\n",
    "    width=[nr_features,16, 8,nr_classes], \n",
    "    grid=5, \n",
    "    k=3, \n",
    "    seed=0\n",
    "    ).to(device) \n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)  # Decay factor of 0.95\n",
    "\n",
    "# Training loop\n",
    "epochs = 30\n",
    "\n",
    "progress_bar = tqdm(range(epochs), desc=\"Training Progress\", unit=\"epoch\")\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU if available\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", lr=f\"{current_lr:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "\n",
    "train_acc = calculate_accuracy(model, train_loader)\n",
    "test_acc = calculate_accuracy(model, test_loader)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import tools\n",
    "\n",
    "class MyKan(KAN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp_dim,\n",
    "        shape,\n",
    "        width,\n",
    "        grid = 5, \n",
    "        k = 3,\n",
    "        act=\"SiLU\",\n",
    "        norm=True,\n",
    "        dist=\"normal\",\n",
    "        std=1.0,\n",
    "        min_std=0.1,\n",
    "        max_std=1.0,\n",
    "        absmax=None,\n",
    "        temp=0.1,\n",
    "        unimix_ratio=0.01,\n",
    "        outscale=1.0,\n",
    "        symlog_inputs=False,\n",
    "        device=\"cuda\",\n",
    "        name=\"NoName\",\n",
    "    ):\n",
    "           \n",
    "        seed = 0\n",
    "        units = width[-1]\n",
    "\n",
    "        super().__init__([inp_dim] + width, grid, k, seed)\n",
    "        inp_dim = width[-1]\n",
    "\n",
    "        self._shape = (shape,) if isinstance(shape, int) else shape\n",
    "        if self._shape is not None and len(self._shape) == 0:\n",
    "            self._shape = (1,)\n",
    "        act = getattr(torch.nn, act)\n",
    "        self._dist = dist\n",
    "        self._std = std if isinstance(std, str) else torch.tensor((std,), device=device)\n",
    "        self._min_std = min_std\n",
    "        self._max_std = max_std\n",
    "        self._absmax = absmax\n",
    "        self._temp = temp\n",
    "        self._unimix_ratio = unimix_ratio\n",
    "        self._symlog_inputs = symlog_inputs\n",
    "        self._device = device\n",
    "\n",
    "        if isinstance(self._shape, dict):\n",
    "            self.mean_layer = nn.ModuleDict()\n",
    "            for name, shape in self._shape.items():\n",
    "                self.mean_layer[name] = nn.Linear(inp_dim, np.prod(shape))\n",
    "            self.mean_layer.apply(tools.uniform_weight_init(outscale))\n",
    "            if self._std == \"learned\":\n",
    "                assert dist in (\"tanh_normal\", \"normal\", \"trunc_normal\", \"huber\"), dist\n",
    "                self.std_layer = nn.ModuleDict()\n",
    "                for name, shape in self._shape.items():\n",
    "                    self.std_layer[name] = nn.Linear(inp_dim, np.prod(shape))\n",
    "                self.std_layer.apply(tools.uniform_weight_init(outscale))\n",
    "        elif self._shape is not None:\n",
    "            self.mean_layer = nn.Linear(inp_dim, np.prod(self._shape))\n",
    "            self.mean_layer.apply(tools.uniform_weight_init(outscale))\n",
    "            if self._std == \"learned\":\n",
    "                assert dist in (\"tanh_normal\", \"normal\", \"trunc_normal\", \"huber\"), dist\n",
    "                self.std_layer = nn.Linear(units, np.prod(self._shape))\n",
    "                self.std_layer.apply(tools.uniform_weight_init(outscale))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = super().forward(x)\n",
    "        out = self.mean_layer(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyKan(\n",
    "    inp_dim=nr_features,\n",
    "    shape=nr_classes,\n",
    "    width=[16, 8],\n",
    "    grid=5,\n",
    "    k=3,\n",
    "    ).to(device)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)  # Decay factor of 0.95\n",
    "\n",
    "# Training loop\n",
    "epochs = 30\n",
    "\n",
    "progress_bar = tqdm(range(epochs), desc=\"Training Progress\", unit=\"epoch\")\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU if available\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", lr=f\"{current_lr:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "\n",
    "train_acc = calculate_accuracy(model, train_loader)\n",
    "test_acc = calculate_accuracy(model, test_loader)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick evaluation with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the MLP that we want to compare to\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(nr_features, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "mlp = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "\n",
    "# train the mlp\n",
    "nr_epochs = 2000\n",
    "for epoch in range(nr_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = mlp(dataset['train_input'])\n",
    "    loss = criterion(output, dataset['train_label'].squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch % (nr_epochs / 10) == 0) or (epoch == nr_epochs - 1):\n",
    "        with torch.no_grad():\n",
    "            train_acc = torch.mean(\n",
    "                (torch.argmax(mlp(dataset['train_input']), dim=1) \n",
    "                == dataset['train_label']).type(dtype))\n",
    "            test_acc = torch.mean(\n",
    "                (torch.argmax(mlp(dataset['test_input']), dim=1) \n",
    "                == dataset['test_label']).type(dtype))\n",
    "\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.5f}, Train Acc: {train_acc:.3f}, Test Acc: {test_acc:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "softmax = np.array([0.2] * 5)\n",
    "print (sum(softmax))\n",
    "entropy = -np.sum(softmax * np.log(softmax))\n",
    "print (softmax, entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = np.array([0.9] + [0.02] * 5)\n",
    "print (sum(softmax))\n",
    "entropy = -np.sum(softmax * np.log(softmax))\n",
    "print (softmax, entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreamer_impera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
